{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FerDataset(Dataset):\n",
    "    def __init__(self, feats, labels, transform):\n",
    "        self.feats = feats\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            idx = idx.tolist()\n",
    "        feat = self.transform(Image.fromarray(np.array(self.feats[index])))\n",
    "        label = torch.tensor(self.labels[idx]).type(torch.long)\n",
    "        return (feat, label)\n",
    "\n",
    "def prepare_data(data):\n",
    "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
    "    image_label = np.array(list(map(int, data['emotion'])))\n",
    "\n",
    "    for i, row in enumerate(data.index):\n",
    "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
    "        image = np.reshape(image, (48, 48))\n",
    "        image_array[i] = image\n",
    "\n",
    "    return image_array, image_label\n",
    "\n",
    "\n",
    "def get_dataloaders(path, batch_size):\n",
    "    fer2013 = pd.read_csv(path)\n",
    "\n",
    "    # [num, 48, 48] [num]\n",
    "    xtrain, ytrain = prepare_data(fer2013[fer2013['Usage'] == 'Training'])\n",
    "    xval, yval = prepare_data(fer2013[fer2013['Usage'] == 'PrivateTest'])\n",
    "    xtest, ytest = prepare_data(fer2013[fer2013['Usage'] == 'PublicTest'])\n",
    "\n",
    "    mu, st = 0, 255\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.TenCrop(40),\n",
    "        transforms.Lambda(lambda crops: torch.stack(\n",
    "            [transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda tensors: torch.stack(\n",
    "            [transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "    ])\n",
    "    train_transform = transforms.Compose([\n",
    "        # 转为灰度图\n",
    "        transforms.Grayscale(),\n",
    "        # 随机裁剪再Resize\n",
    "        transforms.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "        # 修改亮度、对比度、饱和度\n",
    "        transforms.RandomApply([transforms.ColorJitter(\n",
    "            brightness=0.5, contrast=0.5, saturation=0.5)], p=0.5),\n",
    "        # 仿射变换\n",
    "        transforms.RandomApply(\n",
    "            [transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
    "        # 水平翻转\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # 旋转\n",
    "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "        # 上下左右中心裁剪\n",
    "        transforms.FiveCrop(40),\n",
    "\n",
    "        transforms.Lambda(lambda crops: torch.stack(\n",
    "            [transforms.ToTensor()(crop) for crop in crops])),\n",
    "        # 标准化\n",
    "        transforms.Lambda(lambda tensors: torch.stack(\n",
    "            [transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "        # \n",
    "        transforms.Lambda(lambda tensors: torch.stack(\n",
    "            [transforms.RandomErasing()(t) for t in tensors])),\n",
    "    ])\n",
    "    train_transform = test_transform\n",
    "\n",
    "    train = FerDataset(xtrain, ytrain, train_transform)\n",
    "    val = FerDataset(xval, yval, test_transform)\n",
    "    test = FerDataset(xtest, ytest, test_transform)\n",
    "\n",
    "    trainloader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valloader = DataLoader(val, batch_size=64, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(test, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def get_net():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(outputs, smooth_labels):\n",
    "    loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    return loss(F.log_softmax(outputs, dim=1), smooth_labels)\n",
    "\n",
    "\n",
    "def smooth_one_hot(true_labels: torch.Tensor, classes: int, smoothing=0.0):\n",
    "    \"\"\"\n",
    "    if smoothing == 0, it's one-hot method\n",
    "    if 0 < smoothing < 1, it's smooth method\n",
    "\n",
    "    \"\"\"\n",
    "    device = true_labels.device\n",
    "    true_labels = torch.nn.functional.one_hot(\n",
    "        true_labels, classes).detach().cpu()\n",
    "    assert 0 <= smoothing < 1\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = torch.Size((true_labels.size(0), classes))\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.empty(\n",
    "            size=label_shape, device=true_labels.device)\n",
    "        true_dist.fill_(smoothing / (classes - 1))\n",
    "        _, index = torch.max(true_labels, 1)\n",
    "\n",
    "        true_dist.scatter_(1, torch.LongTensor(\n",
    "            index.unsqueeze(1)), confidence)\n",
    "    return true_dist.to(device)\n",
    "\n",
    "\n",
    "class LabelSmoothingLoss(torch.nn.Module):\n",
    "    def __init__(self, smoothing: float = 0.1,\n",
    "                 reduction=\"mean\", weight=None):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.reduction = reduction\n",
    "        self.weight = weight\n",
    "\n",
    "    def reduce_loss(self, loss):\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n",
    "            if self.reduction == 'sum' else loss\n",
    "\n",
    "    def linear_combination(self, x, y):\n",
    "        return self.smoothing * x + (1 - self.smoothing) * y\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        assert 0 <= self.smoothing < 1\n",
    "\n",
    "        if self.weight is not None:\n",
    "            self.weight = self.weight.to(preds.device)\n",
    "\n",
    "        n = preds.size(-1)\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = self.reduce_loss(-log_preds.sum(dim=-1))\n",
    "        nll = F.nll_loss(\n",
    "            log_preds, target, reduction=self.reduction, weight=self.weight\n",
    "        )\n",
    "        return self.linear_combination(loss / n, nll)\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def random_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, loss_fn, optimizer, epoch, device, scaler, writer,\n",
    "        label_smooth, label_smooth_value,mixup,mixup_alpha,Ncrop ):\n",
    "    model.train()\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            if Ncrop:\n",
    "                bs, ncrops, c, h, w = images.shape\n",
    "                images = images.view(-1, c, h, w)\n",
    "                labels = torch.repeat_interleave(labels, repeats=ncrops, dim=0)\n",
    "\n",
    "            if mixup:\n",
    "                images, labels_a, labels_b, lam = mixup_data(\n",
    "                    images, labels, mixup_alpha)\n",
    "                images, labels_a, labels_b = map(\n",
    "                    Variable, (images, labels_a, labels_b))\n",
    "\n",
    "            if epoch == 1:\n",
    "                img_grid = vutils.make_grid(\n",
    "                    images, nrow=10, normalize=True, scale_each=True)\n",
    "                writer.add_image(\"Augemented image\", img_grid, i)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            if label_smooth:\n",
    "                if mixup:\n",
    "                    # mixup + label smooth\n",
    "                    soft_labels_a = smooth_one_hot(\n",
    "                        labels_a, classes=7, smoothing=label_smooth_value)\n",
    "                    soft_labels_b = smooth_one_hot(\n",
    "                        labels_b, classes=7, smoothing=label_smooth_value)\n",
    "                    loss = mixup_criterion(\n",
    "                        loss_fn, outputs, soft_labels_a, soft_labels_b, lam)\n",
    "                else:\n",
    "                    # label smoorth\n",
    "                    soft_labels = smooth_one_hot(\n",
    "                        labels, classes=7, smoothing=label_smooth_value)\n",
    "                    loss = loss_fn(outputs, soft_labels)\n",
    "            else:\n",
    "                if mixup:\n",
    "                    # mixup\n",
    "                    loss = mixup_criterion(\n",
    "                        loss_fn, outputs, labels_a, labels_b, lam)\n",
    "                else:\n",
    "                    # normal CE\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data).item()\n",
    "        count += labels.shape[0]\n",
    "\n",
    "    return train_loss / count, correct / count\n",
    "\n",
    "def evaluate(model, val_loader, device, Ncrop):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if Ncrop:\n",
    "                # fuse crops and batchsize\n",
    "                bs, ncrops, c, h, w = images.shape\n",
    "                images = images.view(-1, c, h, w)\n",
    "\n",
    "                # forward\n",
    "                outputs = model(images)\n",
    "\n",
    "                # combine results across the crops\n",
    "                outputs = outputs.view(bs, ncrops, -1)\n",
    "                outputs = torch.sum(outputs, dim=1) / ncrops\n",
    "\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            val_loss += loss\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data).item()\n",
    "            count += labels.shape[0]\n",
    "\n",
    "        return val_loss / count, correct / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(outputs, smooth_labels):\n",
    "    loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    return loss(F.log_softmax(outputs, dim=1), smooth_labels)\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_name = 'ResNet18'\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "scheduler = 'reduce'\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# todo\n",
    "label_smooth = True \n",
    "label_smooth_value = 0.1\n",
    "mixup = True\n",
    "mixup_alpha = 1.0\n",
    "Ncrop = True\n",
    "\n",
    "# other\n",
    "data_path = 'data/fer13/fer2013.csv'\n",
    "results = './results'\n",
    "writer = SummaryWriter(results+'/tensorboard_log')\n",
    "device = torch.device('cuda:0')\n",
    "save_freq = 10\n",
    "resume = 0\n",
    "seed = 0\n",
    "name = 'official'\n",
    "best_acc = 0.\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "        path=data_path,\n",
    "        bs=batch_size, augment=True)\n",
    "net = get_net()\n",
    "net = net.to(device)\n",
    "\n",
    "loss = cross_entropy\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, \n",
    "        momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(\n",
    "        net, train_loader, loss, optimizer, epoch, device, scaler, writer,\n",
    "        label_smooth, label_smooth_value,mixup,mixup_alpha,Ncrop)\n",
    "    val_loss, val_acc = evaluate(net, val_loader, device, Ncrop)\n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    writer.add_scalar(\"Train/Loss\", train_loss.item(), epoch)\n",
    "    writer.add_scalar(\"Train/Accuracy\", train_acc, epoch)\n",
    "    writer.add_scalar(\"Valid/Loss\", val_loss.item(), epoch)\n",
    "    writer.add_scalar(\"Valid/Accuracy\", val_acc, epoch)\n",
    "\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss.item()}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Train\": train_acc}, epoch)\n",
    "    writer.add_scalars(\"Loss\", {\"Valid\": val_loss.item()}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Valid\": val_acc}, epoch)\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(val_acc, best_acc)\n",
    "    writer.add_scalar(\"Valid/Best Accuracy\", best_acc, epoch)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
